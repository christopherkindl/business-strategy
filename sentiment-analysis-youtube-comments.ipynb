{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSIN0093: Business Strategy and Analytics â€“ Sentiment analysis of YouTube comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [0. Importing dependencies](#0.-Importing-dependencies)\n",
    "* [1. Data scraping with YouTube API](#1.-Data-scraping-with-YouTube-API)\n",
    "* [2. Topic analysis](#2.-Topic-analysis)\n",
    "    * [2a. Data cleaning and pre-processing](#2a.-Data-cleaning-and-pre-processing)\n",
    "    * [2b. One-word frequency distribution](#2b.-One-word-frequency-distribution)\n",
    "    * [2c. Two-word frequency distribution](#2b.-Two-word-frequency-distribution)\n",
    "* [3. Sentiment analysis](#3.-Sentiment-analysis)\n",
    "    * [3a. Data cleaning and pre-processing](#3a.-Data-cleaning-and-pre-processing)\n",
    "    * [3b. Feature identification](#3b.-Feature-identification)\n",
    "    * [3c. Sentiment scoring](#3c.-Sentiment-scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full output rather than just the last line of output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/faculty/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/faculty/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some modules might require pip install\n",
    "# modules for data scraping\n",
    "import os\n",
    "import pickle\n",
    "import google.oauth2.credentials\n",
    "import pandas as pd\n",
    "#from googleapiclient.discovery import build\n",
    "#from googleapiclient.errors import HttpError\n",
    "#from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "#from google.auth.transport.requests import Request\n",
    "#import google.oauth2.credentials\n",
    "\n",
    "\n",
    "# some modules might require pip install\n",
    "# modules for topic and sentiment analysis\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "import nltk.data\n",
    "tokenizer = nltk.downloader.download('punkt')\n",
    "from nltk import ngrams\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data scraping with YouTube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API details\n",
    "client_file = \"client_secret.json\"\n",
    "scopes = ['https://www.googleapis.com/auth/youtube.force-ssl']\n",
    "api_service_name = 'youtube'\n",
    "api_version = 'v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be run on first try in case of issues with authentification\n",
    "\n",
    "def get_authenticated_service():\n",
    "     flow = InstalledAppFlow.from_client_secrets_file(client_file, scopes)\n",
    "     credentials = flow.run_console()\n",
    "     return build(api_service_name, api_version, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # when running locally, disable OAuthlib's HTTPs verification\n",
    "    # when running in production *do not* leave this option enabled\n",
    "    os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n",
    "    service = get_authenticated_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main authentification function and import of libraries and modules\n",
    "\n",
    "def get_authenticated_service():\n",
    "    credentials = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            credentials = pickle.load(token)\n",
    "    #  Check if the credentials are invalid or do not exist\n",
    "    if not credentials or not credentials.valid:\n",
    "        # Check if the credentials have expired\n",
    "        if credentials and credentials.expired and credentials.refresh_token:\n",
    "            credentials.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                client_file, scopes)\n",
    "            credentials = flow.run_console()\n",
    "        # Save the credentials for the next run\n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(credentials, token)\n",
    "        \n",
    "    return build(api_service_name, api_version, credentials = credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions for extracting comments\n",
    "\n",
    "def extract_infos_from_comment(comment,fields=[\"textOriginal\"]):\n",
    "    snippet = comment.get(\"snippet\")\n",
    "    if(snippet):\n",
    "        return( {key:snippet.get(key) for key in fields})\n",
    "    else:\n",
    "        return(None)\n",
    "\n",
    "def get_comment_from_raw_result(result,fields=[\"textOriginal\"]):\n",
    "    main_comment = None\n",
    "    replies = []\n",
    "    snippet = result.get(\"snippet\")\n",
    "    if(snippet):\n",
    "        top_level_comment = snippet.get(\"topLevelComment\")\n",
    "        if(top_level_comment):\n",
    "            main_comment = extract_infos_from_comment(top_level_comment,fields=fields)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    list_replies = result.get(\"replies\")\n",
    "    if(list_replies):\n",
    "        comments = list_replies.get(\"comments\")\n",
    "        if(comments):\n",
    "            for comment in comments:\n",
    "                replies.append(extract_infos_from_comment(comment,fields=fields))\n",
    "    dic = {\"Main_comment\":main_comment,\"replies\":replies}            \n",
    "    return(dic)\n",
    "\n",
    "def get_all_comments_from_results(results,fields=[\"textOriginal\"]):\n",
    "    items = results.get(\"items\")\n",
    "    all_comments = [get_comment_from_raw_result(item,fields=fields) for item in items]\n",
    "    return(all_comments)\n",
    "\n",
    "def get_all_comments(config_request,fields=[\"textOriginal\"],verbose=False):\n",
    "    all_comments = []\n",
    "    service = get_authenticated_service()\n",
    "    results = service.commentThreads().list(**config_request).execute()\n",
    "    current_page = 0\n",
    "    n_total_comments = 0\n",
    "    while results:\n",
    "        current_page += 1\n",
    "        if( verbose):\n",
    "            print(\"parsing comments for page {}..\".format(current_page))\n",
    "        comments_this_page = get_all_comments_from_results(results,fields=fields)\n",
    "        n_comments = len(comments_this_page)\n",
    "        n_total_comments += n_comments\n",
    "        if(verbose):\n",
    "            print(\"Found {} comments on this page..\".format(n_comments))\n",
    "        all_comments.append(comments_this_page)\n",
    "        if 'nextPageToken' in results:\n",
    "                    config_request['pageToken'] = results['nextPageToken']\n",
    "                    results = service.commentThreads().list(**config_request).execute()\n",
    "        else:\n",
    "            break\n",
    "    dic = {\"all_comments\":all_comments,\"n_comments\":n_total_comments}\n",
    "    return(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requesting comments from selected video\n",
    "\n",
    "config_request = {\"part\":\"id,snippet,replies\",\n",
    "                  \"order\":\"time\",\n",
    "                  \"videoId\":\"0wR8-9tjpP4\"} # insert unique video id here\n",
    "\n",
    "fields = [\"textOriginal\",\"publishedAt\",'videoId','authorDisplayName','likeCount']\n",
    "\n",
    "all_comments = get_all_comments(config_request,fields=fields,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_request = {\"part\":\"id,snippet,replies\",\n",
    "                  \"order\":\"time\",\n",
    "                  \"videoId\":\"0wR8-9tjpP4\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through output to build lists for dataframe\n",
    "\n",
    "comments_temp = []\n",
    "comment_time_temp = []\n",
    "video_id_temp = []\n",
    "comment_author_temp = []\n",
    "like_count_temp = []\n",
    "reply_flag_temp = []\n",
    "\n",
    "for full_list in all_comments[\"all_comments\"]:\n",
    "    for element in full_list:\n",
    "        for j in element:\n",
    "            if j == \"Main_comment\":\n",
    "                comments_temp.append(element[\"Main_comment\"][\"textOriginal\"])\n",
    "                comment_time_temp.append(element[\"Main_comment\"][\"publishedAt\"])\n",
    "                video_id_temp.append(element[\"Main_comment\"][\"videoId\"])\n",
    "                comment_author_temp.append(element[\"Main_comment\"][\"authorDisplayName\"])\n",
    "                like_count_temp.append(element[\"Main_comment\"][\"likeCount\"])\n",
    "                reply_flag_temp.append(0)\n",
    "            elif j  == \"replies\":\n",
    "                for reply in element[\"replies\"]:\n",
    "                    comments_temp.append(reply[\"textOriginal\"])\n",
    "                    comment_time_temp.append(reply[\"publishedAt\"])\n",
    "                    video_id_temp.append(reply[\"videoId\"])\n",
    "                    comment_author_temp.append(reply[\"authorDisplayName\"])\n",
    "                    like_count_temp.append(reply[\"likeCount\"])\n",
    "                    reply_flag_temp.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe\n",
    "output_df = pd.DataFrame({'Video ID': video_id_temp,\n",
    "              'Comment': comments_temp,\n",
    "              'Author name': comment_author_temp,\n",
    "              'Timestamp': comment_time_temp,\n",
    "              'Likes': like_count_temp,\n",
    "              'Reply flag': reply_flag_temp})\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting dataframe to csv file\n",
    "video_name = config_request[\"videoId\"]\n",
    "output_df.to_csv(video_name + '_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing file that contains all youtube comments \n",
    "corpus_raw_df = pd.read_csv('all_comments.csv') \n",
    "\n",
    "# importing video list to extract information of which video refers to which brand\n",
    "mapping_df = pd.read_csv('mapping.csv') \n",
    "\n",
    "# join mapping_df into corpus_raw_df\n",
    "corpus_raw_df = pd.merge(corpus_raw_df, mapping_df, on='id', how='left')\n",
    "\n",
    "# drop unnecessary columns\n",
    "corpus_raw_df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'Comment #', 'Video ID', 'Timestamp', 'Likes', 'Reply flag', 'Author name'], inplace= True)\n",
    "\n",
    "# rearranging columns\n",
    "corpus_raw_df = corpus_raw_df.reindex(columns=['id','Comment_id','Comment','brand'])\n",
    "\n",
    "# lowercasing column names\n",
    "corpus_raw_df.columns = map(str.lower, corpus_raw_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function for removing emojis\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. One-word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('watch', 4860),\n",
       " ('apple', 2538),\n",
       " ('thanks', 1921),\n",
       " ('garmin', 1813),\n",
       " ('like', 1538),\n",
       " ('series', 1425),\n",
       " ('review', 1309),\n",
       " ('great', 1308),\n",
       " ('one', 1238),\n",
       " ('fenix', 1221),\n",
       " ('get', 1198),\n",
       " ('would', 1093),\n",
       " ('video', 1015),\n",
       " ('use', 965),\n",
       " ('good', 948),\n",
       " ('battery', 903),\n",
       " ('really', 848),\n",
       " ('still', 754),\n",
       " ('much', 753),\n",
       " ('running', 726),\n",
       " ('think', 702),\n",
       " ('gps', 694),\n",
       " ('also', 694),\n",
       " ('new', 681),\n",
       " ('need', 662),\n",
       " ('better', 652),\n",
       " ('watches', 633),\n",
       " ('want', 630),\n",
       " ('thank', 629),\n",
       " ('always', 629),\n",
       " ('love', 622),\n",
       " ('app', 616),\n",
       " ('time', 601),\n",
       " ('se', 598),\n",
       " ('life', 578),\n",
       " ('phone', 571),\n",
       " ('x', 560),\n",
       " ('best', 557),\n",
       " ('buy', 555),\n",
       " ('run', 552),\n",
       " ('got', 545),\n",
       " ('see', 538),\n",
       " ('know', 536),\n",
       " ('music', 535),\n",
       " ('features', 528),\n",
       " ('pro', 526),\n",
       " ('screen', 514),\n",
       " ('hr', 488),\n",
       " ('go', 473),\n",
       " ('even', 467),\n",
       " ('work', 456),\n",
       " ('sure', 454),\n",
       " ('well', 454),\n",
       " ('nice', 437),\n",
       " ('iphone', 437),\n",
       " ('thing', 424),\n",
       " ('heart', 421),\n",
       " ('day', 410),\n",
       " ('could', 403),\n",
       " ('rate', 395),\n",
       " ('sleep', 384),\n",
       " ('looking', 378),\n",
       " ('getting', 371),\n",
       " ('using', 365),\n",
       " ('desfit', 364),\n",
       " ('lot', 363),\n",
       " ('sapphire', 363),\n",
       " ('display', 363),\n",
       " ('solar', 360),\n",
       " ('first', 359),\n",
       " ('way', 358),\n",
       " ('hi', 353),\n",
       " ('tracking', 348),\n",
       " ('watching', 347),\n",
       " ('fitness', 341),\n",
       " ('upgrade', 340),\n",
       " ('data', 335),\n",
       " ('years', 335),\n",
       " ('keep', 334),\n",
       " ('make', 332),\n",
       " ('wrist', 325),\n",
       " ('people', 317),\n",
       " ('forerunner', 317),\n",
       " ('version', 307),\n",
       " ('feature', 303),\n",
       " ('since', 302),\n",
       " ('connect', 297),\n",
       " ('right', 294),\n",
       " ('price', 294),\n",
       " ('days', 293),\n",
       " ('going', 293),\n",
       " ('accurate', 293),\n",
       " ('yes', 293),\n",
       " ('reviews', 292),\n",
       " ('without', 291),\n",
       " ('plus', 291),\n",
       " ('bought', 290),\n",
       " ('training', 287),\n",
       " ('say', 286),\n",
       " ('year', 286),\n",
       " ('actually', 271),\n",
       " ('something', 267),\n",
       " ('videos', 267),\n",
       " ('look', 267),\n",
       " ('smart', 266),\n",
       " ('pretty', 265),\n",
       " ('every', 265),\n",
       " ('last', 264),\n",
       " ('please', 263),\n",
       " ('used', 263),\n",
       " ('back', 263),\n",
       " ('mm', 261),\n",
       " ('worth', 260),\n",
       " ('though', 255),\n",
       " ('many', 251),\n",
       " ('long', 251),\n",
       " ('never', 244),\n",
       " ('charge', 243),\n",
       " ('sensor', 238),\n",
       " ('bit', 238),\n",
       " ('u', 238),\n",
       " ('big', 236),\n",
       " ('find', 235),\n",
       " ('looks', 235),\n",
       " ('hope', 234),\n",
       " ('nike', 233),\n",
       " ('awesome', 232),\n",
       " ('lol', 224),\n",
       " ('question', 224),\n",
       " ('maps', 220),\n",
       " ('track', 220),\n",
       " ('maybe', 220),\n",
       " ('model', 218),\n",
       " ('far', 215),\n",
       " ('wait', 214),\n",
       " ('help', 213),\n",
       " ('two', 211),\n",
       " ('galaxy', 209),\n",
       " ('different', 208),\n",
       " ('venu', 207),\n",
       " ('device', 206),\n",
       " ('apps', 204),\n",
       " ('fitbit', 202),\n",
       " ('things', 199),\n",
       " ('polar', 198),\n",
       " ('samsung', 198),\n",
       " ('recommend', 197),\n",
       " ('fr', 197),\n",
       " ('workout', 194),\n",
       " ('hours', 193),\n",
       " ('activity', 191),\n",
       " ('instinct', 190),\n",
       " ('size', 190),\n",
       " ('seems', 190),\n",
       " ('hey', 189),\n",
       " ('probably', 189),\n",
       " ('etc', 187),\n",
       " ('android', 187),\n",
       " ('smartwatch', 184),\n",
       " ('money', 184),\n",
       " ('week', 181),\n",
       " ('might', 179),\n",
       " ('old', 178),\n",
       " ('works', 178),\n",
       " ('point', 178),\n",
       " ('take', 177),\n",
       " ('next', 177),\n",
       " ('cellular', 176),\n",
       " ('definitely', 175),\n",
       " ('able', 175),\n",
       " ('everything', 173),\n",
       " ('another', 173),\n",
       " ('support', 172),\n",
       " ('less', 172),\n",
       " ('update', 171),\n",
       " ('around', 171),\n",
       " ('enough', 171),\n",
       " ('already', 170),\n",
       " ('strap', 170),\n",
       " ('https', 169),\n",
       " ('feel', 169),\n",
       " ('give', 168),\n",
       " ('strava', 167),\n",
       " ('difference', 166),\n",
       " ('man', 166),\n",
       " ('glass', 165),\n",
       " ('change', 163),\n",
       " ('mine', 162),\n",
       " ('oxygen', 162),\n",
       " ('max', 161),\n",
       " ('glad', 161),\n",
       " ('accuracy', 160),\n",
       " ('check', 159),\n",
       " ('said', 159),\n",
       " ('band', 159),\n",
       " ('amazing', 157),\n",
       " ('us', 156),\n",
       " ('devices', 155),\n",
       " ('little', 155),\n",
       " ('vivoactive', 154),\n",
       " ('come', 153),\n",
       " ('thinking', 151),\n",
       " ('tell', 151),\n",
       " ('appreciate', 150),\n",
       " ('coming', 150),\n",
       " ('issue', 148),\n",
       " ('reason', 147),\n",
       " ('full', 147),\n",
       " ('may', 146),\n",
       " ('blood', 146),\n",
       " ('activities', 145),\n",
       " ('option', 145),\n",
       " ('buying', 145),\n",
       " ('vs', 144),\n",
       " ('done', 144),\n",
       " ('ever', 143),\n",
       " ('try', 143),\n",
       " ('runs', 143),\n",
       " ('anyone', 143),\n",
       " ('tech', 143),\n",
       " ('face', 142),\n",
       " ('channel', 141),\n",
       " ('wear', 141),\n",
       " ('soon', 141),\n",
       " ('im', 140),\n",
       " ('put', 138),\n",
       " ('aw', 138),\n",
       " ('stuff', 137),\n",
       " ('ago', 137),\n",
       " ('sq', 137),\n",
       " ('monitor', 136),\n",
       " ('cycling', 136),\n",
       " ('found', 136),\n",
       " ('waiting', 136),\n",
       " ('com', 136),\n",
       " ('charging', 135),\n",
       " ('active', 135),\n",
       " ('des', 134),\n",
       " ('chase', 133),\n",
       " ('turn', 133),\n",
       " ('software', 133),\n",
       " ('cool', 132),\n",
       " ('sport', 131),\n",
       " ('summit', 130),\n",
       " ('anything', 129),\n",
       " ('map', 128),\n",
       " ('bad', 127),\n",
       " ('workouts', 127),\n",
       " ('agree', 127),\n",
       " ('yet', 127)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting text/comment into list of sentences\n",
    "corpus_raw = [nltk.sent_tokenize(str(comment)) for comment in corpus_raw_df['comment']]\n",
    "\n",
    "# converting list of lists to one single list\n",
    "corpus_raw = [item for sublist in corpus_raw for item in sublist]\n",
    "\n",
    "# removing to lowercase so that cases, such as \"HR\" and \"hr\", can be matched\n",
    "corpus_raw = [sentence.lower() for sentence in corpus_raw]\n",
    "\n",
    "# removing non-letter characters\n",
    "corpus_raw = [re.sub('[^a-zA-Z]',' ', str(sentence)) for sentence in corpus_raw] \n",
    "\n",
    "# removing emojis\n",
    "corpus_raw = [remove_emoji(sentence) for sentence in corpus_raw]\n",
    "\n",
    "# splitting sentences into list of words\n",
    "corpus_words = [word for sentence in corpus_raw for word in sentence.split()]\n",
    "\n",
    "# removing stopwords (e.g. \"and\", \"he\", \"are\") \n",
    "corpus_words = [word for word in corpus_words if word not in stop_words]\n",
    "\n",
    "# creating dictionary of all words from corpus with their counts\n",
    "words_counts = {}\n",
    "for word in corpus_words:\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 1\n",
    "        words_counts[word] += 1\n",
    "        \n",
    "# sorting dictionary from highest to lowest value \n",
    "bag_of_words_ranked = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# showing the 250 most frequent words\n",
    "bag_of_words_ranked[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c. Two-word frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('apple', 'watch'), 1609),\n",
       " (('battery', 'life'), 479),\n",
       " (('heart', 'rate'), 362),\n",
       " (('great', 'review'), 314),\n",
       " (('watch', 'series'), 234),\n",
       " (('great', 'video'), 222),\n",
       " (('thanks', 'watching'), 213),\n",
       " (('fenix', 'x'), 191),\n",
       " (('always', 'display'), 183),\n",
       " (('garmin', 'connect'), 158),\n",
       " (('sleep', 'tracking'), 156),\n",
       " (('galaxy', 'watch'), 150),\n",
       " (('smart', 'watch'), 142),\n",
       " (('watch', 'se'), 137),\n",
       " (('sure', 'thing'), 136),\n",
       " (('chase', 'summit'), 130),\n",
       " (('x', 'pro'), 121),\n",
       " (('blood', 'oxygen'), 112),\n",
       " (('garmin', 'fenix'), 104),\n",
       " (('fenix', 'pro'), 101),\n",
       " (('venu', 'sq'), 101),\n",
       " (('thanks', 'much'), 98),\n",
       " (('thanks', 'great'), 93),\n",
       " (('watch', 'face'), 83),\n",
       " (('would', 'like'), 82),\n",
       " (('would', 'recommend'), 81),\n",
       " (('much', 'better'), 78),\n",
       " (('vo', 'max'), 77),\n",
       " (('hope', 'helps'), 76),\n",
       " (('dc', 'rainmaker'), 76),\n",
       " (('thanks', 'review'), 76),\n",
       " (('desfit', 'thanks'), 76),\n",
       " (('review', 'thanks'), 75),\n",
       " (('pro', 'solar'), 72),\n",
       " (('x', 'plus'), 71),\n",
       " (('thank', 'much'), 71),\n",
       " (('first', 'apple'), 69),\n",
       " (('looks', 'like'), 68),\n",
       " (('stainless', 'steel'), 68),\n",
       " (('running', 'watch'), 67),\n",
       " (('watch', 'would'), 67),\n",
       " (('apple', 'watches'), 67),\n",
       " (('https', 'www'), 63),\n",
       " (('smart', 'watches'), 62),\n",
       " (('nice', 'review'), 60),\n",
       " (('fenix', 'hr'), 60),\n",
       " (('fenix', 'plus'), 60),\n",
       " (('garmin', 'watch'), 59),\n",
       " (('without', 'phone'), 59),\n",
       " (('watch', 'faces'), 59),\n",
       " (('good', 'review'), 58),\n",
       " (('rate', 'monitor'), 58),\n",
       " (('garmin', 'forerunner'), 58),\n",
       " (('really', 'like'), 57),\n",
       " (('watch', 'apple'), 56),\n",
       " (('looking', 'forward'), 55),\n",
       " (('new', 'watch'), 54),\n",
       " (('great', 'watch'), 54),\n",
       " (('thing', 'desfit'), 53),\n",
       " (('seems', 'like'), 52),\n",
       " (('gps', 'watch'), 52),\n",
       " (('nike', 'run'), 52),\n",
       " (('chest', 'strap'), 51),\n",
       " (('use', 'apple'), 51),\n",
       " (('x', 'sapphire'), 49),\n",
       " (('keep', 'muscle'), 49),\n",
       " (('muscle', 'mass'), 49),\n",
       " (('mass', 'holidays'), 49),\n",
       " (('nacho', 'cheese'), 49),\n",
       " (('watch', 'great'), 48),\n",
       " (('thanks', 'video'), 48),\n",
       " (('video', 'thanks'), 48),\n",
       " (('long', 'term'), 48),\n",
       " (('sapphire', 'glass'), 47),\n",
       " (('connect', 'iq'), 47),\n",
       " (('feel', 'like'), 47),\n",
       " (('glad', 'helped'), 47),\n",
       " (('third', 'party'), 46),\n",
       " (('years', 'ago'), 46),\n",
       " (('use', 'watch'), 46),\n",
       " (('watch', 'like'), 46),\n",
       " (('buy', 'apple'), 46),\n",
       " (('really', 'good'), 45),\n",
       " (('desfit', 'thank'), 45),\n",
       " (('pretty', 'much'), 44),\n",
       " (('garmin', 'watches'), 44),\n",
       " (('garmin', 'instinct'), 44),\n",
       " (('fenix', 'sapphire'), 44),\n",
       " (('gorilla', 'glass'), 44),\n",
       " (('watch', 'good'), 44),\n",
       " (('get', 'one'), 44),\n",
       " (('rd', 'party'), 44),\n",
       " (('run', 'club'), 44),\n",
       " (('like', 'apple'), 43),\n",
       " (('data', 'fields'), 43),\n",
       " (('touch', 'screen'), 43),\n",
       " (('series', 'series'), 43),\n",
       " (('good', 'work'), 42),\n",
       " (('every', 'day'), 42),\n",
       " (('https', 'youtu'), 42),\n",
       " (('review', 'thank'), 42),\n",
       " (('fitness', 'watch'), 42),\n",
       " (('nice', 'video'), 42),\n",
       " (('samsung', 'galaxy'), 42),\n",
       " (('keep', 'good'), 41),\n",
       " (('watch', 'thanks'), 41),\n",
       " (('would', 'love'), 41),\n",
       " (('series', 'se'), 40),\n",
       " (('watch', 'active'), 40),\n",
       " (('cheese', 'dispenser'), 40),\n",
       " (('rate', 'sensor'), 39),\n",
       " (('watch', 'garmin'), 39),\n",
       " (('get', 'apple'), 39),\n",
       " (('watch', 'still'), 39),\n",
       " (('thing', 'thanks'), 39),\n",
       " (('really', 'need'), 38),\n",
       " (('best', 'watch'), 38),\n",
       " (('really', 'want'), 38),\n",
       " (('new', 'features'), 38),\n",
       " (('upgrade', 'series'), 38),\n",
       " (('workout', 'app'), 38),\n",
       " (('hr', 'sensor'), 37),\n",
       " (('garmin', 'venu'), 37),\n",
       " (('watch', 'battery'), 36),\n",
       " (('use', 'garmin'), 36),\n",
       " (('better', 'battery'), 36),\n",
       " (('excellent', 'review'), 36),\n",
       " (('fitbit', 'versa'), 36),\n",
       " (('something', 'like'), 35),\n",
       " (('thanks', 'reply'), 34),\n",
       " (('grit', 'x'), 34),\n",
       " (('love', 'apple'), 34),\n",
       " (('watch', 'get'), 34),\n",
       " (('every', 'year'), 34),\n",
       " (('pretty', 'good'), 34),\n",
       " (('really', 'appreciate'), 34),\n",
       " (('cellular', 'version'), 34),\n",
       " (('hr', 'monitor'), 34),\n",
       " (('always', 'screen'), 34),\n",
       " (('watch', 'app'), 33),\n",
       " (('great', 'job'), 33),\n",
       " (('series', 'apple'), 33),\n",
       " (('one', 'would'), 33),\n",
       " (('run', 'testers'), 33),\n",
       " (('want', 'watch'), 32),\n",
       " (('mm', 'mm'), 32),\n",
       " (('party', 'apps'), 32),\n",
       " (('watch', 'use'), 31),\n",
       " (('thanks', 'thanks'), 31),\n",
       " (('long', 'time'), 31),\n",
       " (('watch', 'really'), 31),\n",
       " (('next', 'year'), 31),\n",
       " (('buy', 'watch'), 31),\n",
       " (('thanks', 'lot'), 31),\n",
       " (('got', 'series'), 31),\n",
       " (('apple', 'products'), 31),\n",
       " (('let', 'know'), 31),\n",
       " (('want', 'buy'), 31),\n",
       " (('like', 'garmin'), 30),\n",
       " (('good', 'enough'), 30),\n",
       " (('gps', 'accuracy'), 30),\n",
       " (('buy', 'one'), 30),\n",
       " (('oxygen', 'sensor'), 30),\n",
       " (('day', 'battery'), 30),\n",
       " (('fitbit', 'sense'), 30),\n",
       " (('would', 'great'), 29),\n",
       " (('music', 'watch'), 29),\n",
       " (('vantage', 'v'), 29),\n",
       " (('one', 'question'), 29),\n",
       " (('watch', 'also'), 29),\n",
       " (('sounds', 'like'), 29),\n",
       " (('thinking', 'getting'), 29),\n",
       " (('new', 'one'), 29),\n",
       " (('bought', 'fenix'), 29),\n",
       " (('upgrading', 'series'), 29),\n",
       " (('coming', 'soon'), 29),\n",
       " (('love', 'videos'), 29),\n",
       " (('get', 'new'), 29),\n",
       " (('connect', 'app'), 29),\n",
       " (('getting', 'one'), 29),\n",
       " (('last', 'years'), 29),\n",
       " (('awesome', 'review'), 29),\n",
       " (('apple', 'music'), 29),\n",
       " (('watch', 'os'), 29),\n",
       " (('nike', 'edition'), 29),\n",
       " (('running', 'app'), 29),\n",
       " (('app', 'store'), 29),\n",
       " (('year', 'old'), 29),\n",
       " (('helped', 'thanks'), 29),\n",
       " (('trail', 'running'), 28),\n",
       " (('get', 'garmin'), 28),\n",
       " (('fitness', 'tracker'), 28),\n",
       " (('would', 'get'), 28),\n",
       " (('fenix', 'series'), 28),\n",
       " (('thanks', 'advance'), 28),\n",
       " (('polar', 'vantage'), 28),\n",
       " (('every', 'time'), 28),\n",
       " (('full', 'review'), 28),\n",
       " (('want', 'know'), 28),\n",
       " (('even', 'though'), 27),\n",
       " (('buy', 'garmin'), 27),\n",
       " (('open', 'water'), 27),\n",
       " (('well', 'done'), 27),\n",
       " (('watch', 'one'), 27),\n",
       " (('makes', 'sense'), 27),\n",
       " (('great', 'vid'), 27),\n",
       " (('review', 'always'), 27),\n",
       " (('pulse', 'ox'), 27),\n",
       " (('screen', 'protector'), 27),\n",
       " (('little', 'bit'), 27),\n",
       " (('best', 'review'), 27),\n",
       " (('hi', 'des'), 27),\n",
       " (('best', 'smartwatch'), 27),\n",
       " (('watch', 'running'), 27),\n",
       " (('iphone', 'se'), 27),\n",
       " (('first', 'time'), 26),\n",
       " (('pro', 'sapphire'), 26),\n",
       " (('running', 'cycling'), 26),\n",
       " (('watch', 'always'), 26),\n",
       " (('really', 'nice'), 26),\n",
       " (('wear', 'watch'), 26),\n",
       " (('one', 'thing'), 26),\n",
       " (('would', 'say'), 26),\n",
       " (('weight', 'training'), 26),\n",
       " (('get', 'better'), 26),\n",
       " (('iphone', 'pro'), 26),\n",
       " (('got', 'one'), 26),\n",
       " (('getting', 'apple'), 26),\n",
       " (('one', 'best'), 26),\n",
       " (('samsung', 'watch'), 26),\n",
       " (('nike', 'version'), 26),\n",
       " (('solar', 'version'), 26),\n",
       " (('thanks', 'desfit'), 26),\n",
       " (('depth', 'review'), 26),\n",
       " (('garmin', 'vivoactive'), 26),\n",
       " (('club', 'app'), 26),\n",
       " (('se', 'nike'), 26),\n",
       " (('hikingguy', 'com'), 26),\n",
       " (('trying', 'decide'), 25),\n",
       " (('love', 'watch'), 25),\n",
       " (('get', 'watch'), 25),\n",
       " (('make', 'sure'), 25),\n",
       " (('like', 'watch'), 25),\n",
       " (('fenix', 'line'), 25),\n",
       " (('got', 'mine'), 25),\n",
       " (('listen', 'music'), 25),\n",
       " (('watch', 'work'), 25),\n",
       " (('holidays', 'https'), 25),\n",
       " (('www', 'news'), 25),\n",
       " (('news', 'sport'), 25)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# splitting sentences into n-grams word combinations (e.g. n=2, \"hr monitor\")\n",
    "n = 2 # testing for 2-words combinations\n",
    "corpus_ngram = list(ngrams(corpus_words, n))\n",
    "\n",
    "# creating dictionary of all words from corpus with their counts.\n",
    "ngram_counts = {}\n",
    "for word in corpus_ngram:\n",
    "        if word not in ngram_counts:\n",
    "            ngram_counts[word] = 1\n",
    "        ngram_counts[word] += 1\n",
    "        \n",
    "# sorting dictionary from highest to lowest value\n",
    "ngram_ranked = sorted(ngram_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# showing the ten most frequent n-words combination\n",
    "ngram_ranked[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Data cleaning and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments in Garmin subset: 6878\n",
      "Number of comments in Apple subset: 7431\n",
      "Number of sentences in Apple subset: 15734\n",
      "Number of sentences in Garmin subset: 14718\n"
     ]
    }
   ],
   "source": [
    "# creating subset for each brand and applying little data cleaning\n",
    "\n",
    "# ------------------- SUBSET GARMIN -------------------\n",
    "\n",
    "# creating subset for Garmin-related comments\n",
    "corpus_garmin = corpus_raw_df[corpus_raw_df['brand'] == 'Garmin'] \n",
    "\n",
    "# reporting number of comments before comment to sentence transformation\n",
    "print(\"Number of comments in Garmin subset:\", len(corpus_garmin))\n",
    "\n",
    "# splitting text/comment into list of sentences\n",
    "corpus_garmin = [nltk.sent_tokenize(str(comment)) for comment in corpus_garmin[\"comment\"]]\n",
    "\n",
    "# converting list of lists to one single list\n",
    "corpus_garmin = [item for sublist in corpus_garmin for item in sublist]\n",
    "\n",
    "# lowercase for better matching with keyword lists\n",
    "corpus_garmin = [sentence.lower() for sentence in corpus_garmin] \n",
    "\n",
    "\n",
    "# ------------------- SUBSET APPLE -------------------\n",
    "\n",
    "# creating subset for Apple-related comments\n",
    "corpus_apple = corpus_raw_df[corpus_raw_df['brand'] == 'Apple'] \n",
    "\n",
    "# reporting number of comments before comment to sentence transformation\n",
    "print(\"Number of comments in Apple subset:\", len(corpus_apple))\n",
    "\n",
    "# splitting text/comment into list of sentences\n",
    "corpus_apple = [nltk.sent_tokenize(str(comment)) for comment in corpus_apple[\"comment\"]]\n",
    "\n",
    "# converting list of lists to one single list\n",
    "corpus_apple = [item for sublist in corpus_apple for item in sublist]\n",
    "\n",
    "# lowercase for better matching with keyword lists\n",
    "corpus_apple = [sentence.lower() for sentence in corpus_apple] \n",
    "\n",
    "# reporting number of comments after comment to sentence transformation\n",
    "print(\"Number of sentences in Apple subset:\", len(corpus_garmin))\n",
    "print(\"Number of sentences in Garmin subset:\", len(corpus_apple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3b. Feature-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# defining lists of associated keywords for each feature\n",
    "keywords_topic_gps = ['gps', 'geolocation']\n",
    "keywords_topic_hr = ['hr', 'heart rate', 'hr', 'bpm']\n",
    "keywords_topic_battery = ['battery', 'battery life', 'charging', 'battery lifetime']\n",
    "keywords_topic_vo2 = ['vo2', 'pulseox', 'oxygen']\n",
    "keywords_topic_price = ['budget', 'overpriced', 'expensive', 'affordable', 'dollar', \"$\" 'dollars', 'cheap', 'â‚¬', 'euro', 'euros']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying feature-related sentences for Apple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences before removing questions: 14718\n",
      "number of sentences after removing question marks: 12847\n"
     ]
    }
   ],
   "source": [
    "# removing sentences with question marks\n",
    "to_delete = []\n",
    "for sentence in corpus_apple:\n",
    "    for word in sentence:\n",
    "        if word == \"?\":\n",
    "            to_delete.append(sentence)\n",
    "\n",
    "corpus_apple_2 = []\n",
    "\n",
    "for sentence in corpus_apple:\n",
    "    if sentence not in to_delete:\n",
    "        corpus_apple_2.append(sentence)\n",
    "        \n",
    "# reporting number of sentences before and after removing questions\n",
    "print(\"number of sentences before removing questions:\", len(corpus_apple))\n",
    "print(\"number of sentences after removing question marks:\", len(corpus_apple_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences after removing non-Apple brand related sentences: 12306\n",
      "Number of sentences after one-word matching: 808\n",
      "Number of sentences after two-word matching: 860\n",
      "Number of sentences after validation: 860\n",
      "Number of sentences for gps: 135\n",
      "Number of sentences for hr: 94\n",
      "Number of sentences for battery: 419\n",
      "Number of sentences for vo2: 136\n",
      "Number of sentences for price: 76\n"
     ]
    }
   ],
   "source": [
    "# removing sentences that refer to other brands\n",
    "non_apple = ['forerunner', 'garmin', 'vivoactive', 'huawei', 'polar', 'fenix', 'withings', 'suunto', 'xiaomi', 'fitbit']\n",
    "to_delete_2 = []\n",
    "non_apple_count = 0\n",
    "\n",
    "for sentence in corpus_apple_2:\n",
    "    for word in sentence.split():\n",
    "        if word in non_apple:\n",
    "                non_apple_count += 1\n",
    "                to_delete_2.append(sentence)  \n",
    "\n",
    "corpus_apple_3 = []\n",
    "\n",
    "for sentence in corpus_apple_2:\n",
    "    if sentence not in to_delete_2:\n",
    "        corpus_apple_3.append(sentence)\n",
    "\n",
    "# reporting number of sentences after removing non-Apple brand related sentences\n",
    "print(\"number of sentences after removing non-Apple brand related sentences:\", len(corpus_apple_3)) \n",
    "\n",
    "\n",
    "# assigning sentences of a review to a topic list if a word in the sentence matches a keyword of a topic list\n",
    "app_assigned_to_topic_gps = []\n",
    "app_assigned_to_topic_hr = []\n",
    "app_assigned_to_topic_battery = []\n",
    "app_assigned_to_topic_vo2 = []\n",
    "app_assigned_to_topic_price = []\n",
    "\n",
    "for sentence in corpus_apple_3:\n",
    "    for word in sentence.split():\n",
    "        if word in keywords_topic_hr:\n",
    "            app_assigned_to_topic_hr.append(sentence)\n",
    "        if word in keywords_topic_gps:\n",
    "            app_assigned_to_topic_gps.append(sentence)\n",
    "        if word in keywords_topic_battery:\n",
    "            app_assigned_to_topic_battery.append(sentence)\n",
    "        if word in keywords_topic_vo2:\n",
    "            app_assigned_to_topic_vo2.append(sentence)\n",
    "        if word in keywords_topic_price:\n",
    "            app_assigned_to_topic_price.append(sentence)\n",
    "\n",
    "                       \n",
    "# reporting number of extracted sentences after one-word matching\n",
    "app_sentences_one_w = len(app_assigned_to_topic_hr) + len(app_assigned_to_topic_gps) + len(app_assigned_to_topic_battery) + len(app_assigned_to_topic_vo2) + len(app_assigned_to_topic_price) \n",
    "                \n",
    "print(\"Number of sentences after one-word matching:\", app_sentences_one_w)\n",
    "\n",
    "# matching with two-word combinations\n",
    "for sentence in corpus_apple_3:\n",
    "    for grams in ngrams(sentence.split(), 2): # creating two word combination\n",
    "        if ' '.join(grams) in keywords_topic_hr:\n",
    "            if sentence not in app_assigned_to_topic_hr:\n",
    "                app_assigned_to_topic_hr.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_gps:\n",
    "            if sentence not in app_assigned_to_topic_gps:\n",
    "                app_assigned_to_topic_gps.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_battery:\n",
    "            if sentence not in app_assigned_to_topic_battery:\n",
    "                app_assigned_to_topic_battery.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_vo2:\n",
    "            if sentence not in app_assigned_to_topic_vo2:\n",
    "                app_assigned_to_topic_vo2.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_price:\n",
    "            if sentence not in app_assigned_to_topic_price:\n",
    "                app_assigned_to_topic_price.append(sentence)\n",
    "                \n",
    "app_sentences_two_w = len(app_assigned_to_topic_hr) + len(app_assigned_to_topic_gps) + len(app_assigned_to_topic_battery) + len(app_assigned_to_topic_vo2) + len(app_assigned_to_topic_price) \n",
    "                \n",
    "print(\"Number of sentences after two-word matching:\", app_sentences_two_w)\n",
    "\n",
    "# screening list of assigned sentences of topic gps\n",
    "gps_fraud = []\n",
    "for sentence in app_assigned_to_topic_gps:\n",
    "    if sentence in app_assigned_to_topic_hr or app_assigned_to_topic_battery or app_assigned_to_topic_vo2 or app_assigned_to_topic_price:\n",
    "        gps_fraud.append(sentence)\n",
    "\n",
    "\n",
    "# screening list of assigned sentences to topic hr\n",
    "hr_fraud = []\n",
    "for sentence in app_assigned_to_topic_hr:\n",
    "    if sentence in app_assigned_to_topic_gps or app_assigned_to_topic_battery or app_assigned_to_topic_vo2 or app_assigned_to_topic_price:\n",
    "        hr_fraud.append(sentence)\n",
    "\n",
    "# screening list of assigned sentences to topic battery\n",
    "battery_fraud = []\n",
    "for sentence in app_assigned_to_topic_battery:\n",
    "    if sentence in app_assigned_to_topic_hr or app_assigned_to_topic_gps or app_assigned_to_topic_vo2 or app_assigned_to_topic_price:\n",
    "        battery_fraud.append(sentence)    \n",
    "        \n",
    "# screening list of assigned sentences to topic vo2\n",
    "vo2_fraud = []\n",
    "for sentence in app_assigned_to_topic_vo2:\n",
    "    if sentence in app_assigned_to_topic_hr or app_assigned_to_topic_gps or app_assigned_to_topic_battery or app_assigned_to_topic_price:\n",
    "        vo2_fraud.append(sentence) \n",
    "        \n",
    "# screening list of assigned sentences to topic price\n",
    "price_fraud = []\n",
    "for sentence in app_assigned_to_topic_price:\n",
    "    if sentence in app_assigned_to_topic_hr or app_assigned_to_topic_gps or app_assigned_to_topic_battery or app_assigned_to_topic_vo2:\n",
    "           price_fraud.append(sentence) \n",
    "    \n",
    "      \n",
    "# reporting number of sentences after validation\n",
    "app_sentences_after = len(app_assigned_to_topic_gps) + len(app_assigned_to_topic_hr) + len(app_assigned_to_topic_battery) + len(app_assigned_to_topic_vo2) + len(app_assigned_to_topic_price) \n",
    "             \n",
    "print(\"Number of sentences after validation:\", app_sentences_after)\n",
    "\n",
    "# reporting number of sentences for each feature topic\n",
    "print(\"Number of sentences for gps:\", len(app_assigned_to_topic_gps))\n",
    "print(\"Number of sentences for hr:\", len(app_assigned_to_topic_hr))\n",
    "print(\"Number of sentences for battery:\", len(app_assigned_to_topic_battery))\n",
    "print(\"Number of sentences for vo2:\", len(app_assigned_to_topic_vo2))\n",
    "print(\"Number of sentences for price:\", len(app_assigned_to_topic_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exporting data for Apple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments after removing duplicates: 860\n",
      "Number of comments after removing duplicates: 772\n",
      "Number of sentences for gps: 122\n",
      "Number of sentences for hr: 80\n",
      "Number of sentences for battery: 378\n",
      "Number of sentences for vo2: 125\n",
      "Number of sentences for price: 67\n"
     ]
    }
   ],
   "source": [
    "# create dataframe for each list\n",
    "app_gps_df = pd.DataFrame(app_assigned_to_topic_gps, columns = ['comment'])\n",
    "app_hr_df = pd.DataFrame(app_assigned_to_topic_hr, columns = ['comment'])\n",
    "app_battery_df = pd.DataFrame(app_assigned_to_topic_battery, columns = ['comment'])\n",
    "app_vo2_df = pd.DataFrame(app_assigned_to_topic_vo2, columns = ['comment'])\n",
    "app_price_df = pd.DataFrame(app_assigned_to_topic_price, columns = ['comment'])\n",
    "\n",
    "# add label column for topic\n",
    "app_gps_df['topic'] = 'gps'\n",
    "app_hr_df['topic'] = 'hr'\n",
    "app_battery_df['topic'] = 'battery'\n",
    "app_vo2_df['topic'] = 'vo2'\n",
    "app_price_df['topic'] = 'price'\n",
    "\n",
    "# join dataframes together\n",
    "frames = [app_gps_df,\n",
    "          app_hr_df,\n",
    "          app_battery_df,\n",
    "          app_vo2_df,\n",
    "          app_price_df]\n",
    "\n",
    "\n",
    "apple_topic_comments = pd.concat(frames)\n",
    "\n",
    "# reporting number of sentences before removing duplicates\n",
    "print(\"Number of comments after removing duplicates:\", len(apple_topic_comments))\n",
    "\n",
    "# removing duplicates to ensure one sentence is not assigned to more than one feature topic\n",
    "apple_topic_comments.drop_duplicates(subset=['comment'], inplace = True)\n",
    "\n",
    "# reporting number of sentences after removing duplicates\n",
    "print(\"Number of comments after removing duplicates:\", len(apple_topic_comments))\n",
    "\n",
    "# reporting number of sentences for each feature topic\n",
    "print(\"Number of sentences for gps:\", len(apple_topic_comments[apple_topic_comments['topic'] == 'gps']))\n",
    "print(\"Number of sentences for hr:\", len(apple_topic_comments[apple_topic_comments['topic'] == 'hr']))\n",
    "print(\"Number of sentences for battery:\", len(apple_topic_comments[apple_topic_comments['topic'] == 'battery']))\n",
    "print(\"Number of sentences for vo2:\", len(apple_topic_comments[apple_topic_comments['topic'] == 'vo2']))\n",
    "print(\"Number of sentences for price:\", len(apple_topic_comments[apple_topic_comments['topic'] == 'price']))\n",
    "\n",
    "# export to csv\n",
    "date = '2020-12-08'\n",
    "apple_topic_comments.to_csv(date + '_apple_topic_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying feature-related sentences for Garmin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences before removing questions: 15734\n",
      "number of sentences after removing question marks: 13020\n"
     ]
    }
   ],
   "source": [
    "# removing sentences with question marks\n",
    "to_delete = []\n",
    "for sentence in corpus_garmin:\n",
    "    for word in sentence:\n",
    "        if word == \"?\":\n",
    "            to_delete.append(sentence)\n",
    "\n",
    "corpus_garmin_2 = []\n",
    "\n",
    "for sentence in corpus_garmin:\n",
    "    if sentence not in to_delete:\n",
    "        corpus_garmin_2.append(sentence)\n",
    "        \n",
    "# reporting number of sentences before and after removing questions\n",
    "print(\"number of sentences before removing questions:\", len(corpus_garmin))\n",
    "print(\"number of sentences after removing question marks:\", len(corpus_garmin_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentences after removing non-Apple brand related sentences: 12645\n",
      "Number of sentences after one-word matching: 846\n",
      "Number of sentences after two-word matching: 959\n",
      "Number of sentences after validation: 959\n",
      "Number of sentences for gps: 238\n",
      "Number of sentences for hr: 323\n",
      "Number of sentences for battery: 279\n",
      "Number of sentences for vo2: 29\n",
      "Number of sentences for price: 90\n"
     ]
    }
   ],
   "source": [
    "# removing sentences that refer to other brands\n",
    "non_garmin = ['apple','huawei', 'polar', 'withings', 'suunto', 'xiaomi', 'fitbit']\n",
    "to_delete_3 = []\n",
    "non_garmin_count = 0\n",
    "\n",
    "for sentence in corpus_garmin_2:\n",
    "    for word in sentence.split():\n",
    "        if word in non_garmin:\n",
    "                non_garmin_count += 1\n",
    "                to_delete_3.append(sentence)  \n",
    "\n",
    "corpus_garmin_3 = []\n",
    "\n",
    "for sentence in corpus_garmin_2:\n",
    "    if sentence not in to_delete_3:\n",
    "        corpus_garmin_3.append(sentence)\n",
    "\n",
    "# reporting number of sentences after removing non-Garmin brand related sentences\n",
    "print(\"number of sentences after removing non-Garmin brand related sentences:\", len(corpus_garmin_3)) \n",
    "\n",
    "\n",
    "# assigning sentences of a review to a topic list if a word in the sentence matches a keyword of a topic list\n",
    "gar_assigned_to_topic_gps = []\n",
    "gar_assigned_to_topic_hr = []\n",
    "gar_assigned_to_topic_battery = []\n",
    "gar_assigned_to_topic_vo2 = []\n",
    "gar_assigned_to_topic_price = []\n",
    "\n",
    "for sentence in corpus_garmin_3:\n",
    "    for word in sentence.split():\n",
    "        if word in keywords_topic_hr:\n",
    "            gar_assigned_to_topic_hr.append(sentence)\n",
    "        if word in keywords_topic_gps:\n",
    "            gar_assigned_to_topic_gps.append(sentence)\n",
    "        if word in keywords_topic_battery:\n",
    "            gar_assigned_to_topic_battery.append(sentence)\n",
    "        if word in keywords_topic_vo2:\n",
    "            gar_assigned_to_topic_vo2.append(sentence)\n",
    "        if word in keywords_topic_price:\n",
    "            gar_assigned_to_topic_price.append(sentence)\n",
    "\n",
    "                       \n",
    "# reporting number of extracted sentences after one-word matching\n",
    "gar_sentences_one_w = len(gar_assigned_to_topic_hr) + len(gar_assigned_to_topic_gps) + len(gar_assigned_to_topic_battery) + len(gar_assigned_to_topic_vo2) + len(gar_assigned_to_topic_price) \n",
    "                \n",
    "print(\"Number of sentences after one-word matching:\", gar_sentences_one_w)\n",
    "\n",
    "# matching with two-word combinations\n",
    "for sentence in corpus_garmin_3:\n",
    "    for grams in ngrams(sentence.split(), 2): # creating two word combination\n",
    "        if ' '.join(grams) in keywords_topic_hr:\n",
    "            if sentence not in gar_assigned_to_topic_hr:\n",
    "                gar_assigned_to_topic_hr.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_gps:\n",
    "            if sentence not in gar_assigned_to_topic_gps:\n",
    "                gar_assigned_to_topic_gps.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_battery:\n",
    "            if sentence not in gar_assigned_to_topic_battery:\n",
    "                gar_assigned_to_topic_battery.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_vo2:\n",
    "            if sentence not in gar_assigned_to_topic_vo2:\n",
    "                gar_assigned_to_topic_vo2.append(sentence)\n",
    "        if ' '.join(grams) in keywords_topic_price:\n",
    "            if sentence not in gar_assigned_to_topic_price:\n",
    "                gar_assigned_to_topic_price.append(sentence)\n",
    "                \n",
    "gar_sentences_two_w = len(gar_assigned_to_topic_hr) + len(gar_assigned_to_topic_gps) + len(gar_assigned_to_topic_battery) + len(gar_assigned_to_topic_vo2) + len(gar_assigned_to_topic_price) \n",
    "                \n",
    "print(\"Number of sentences after two-word matching:\", gar_sentences_two_w)\n",
    "\n",
    "# screening list of assigned sentences of topic gps\n",
    "gar_gps_fraud = []\n",
    "for sentence in gar_assigned_to_topic_gps:\n",
    "    if sentence in gar_assigned_to_topic_hr or gar_assigned_to_topic_battery or gar_assigned_to_topic_vo2 or gar_assigned_to_topic_price:\n",
    "        gar_gps_fraud.append(sentence)\n",
    "\n",
    "\n",
    "# screening list of assigned sentences to topic hr\n",
    "gar_hr_fraud = []\n",
    "for sentence in gar_assigned_to_topic_hr:\n",
    "    if sentence in gar_assigned_to_topic_gps or gar_assigned_to_topic_battery or gar_assigned_to_topic_vo2 or gar_assigned_to_topic_price:\n",
    "        gar_hr_fraud.append(sentence)\n",
    "\n",
    "# screening list of assigned sentences to topic battery\n",
    "gar_battery_fraud = []\n",
    "for sentence in gar_assigned_to_topic_battery:\n",
    "    if sentence in gar_assigned_to_topic_hr or gar_assigned_to_topic_gps or gar_assigned_to_topic_vo2 or gar_assigned_to_topic_price:\n",
    "        gar_battery_fraud.append(sentence)    \n",
    "        \n",
    "# screening list of assigned sentences to topic vo2\n",
    "gar_vo2_fraud = []\n",
    "for sentence in gar_assigned_to_topic_vo2:\n",
    "    if sentence in gar_assigned_to_topic_hr or gar_assigned_to_topic_gps or gar_assigned_to_topic_battery or gar_assigned_to_topic_price:\n",
    "        gar_vo2_fraud.append(sentence) \n",
    "        \n",
    "# screening list of assigned sentences to topic price\n",
    "gar_price_fraud = []\n",
    "for sentence in gar_assigned_to_topic_price:\n",
    "    if sentence in gar_assigned_to_topic_hr or gar_assigned_to_topic_gps or gar_assigned_to_topic_battery or gar_assigned_to_topic_vo2:\n",
    "           gar_price_fraud.append(sentence) \n",
    "    \n",
    "      \n",
    "# reporting number of sentences after validation\n",
    "gar_sentences_after = len(gar_assigned_to_topic_gps) + len(gar_assigned_to_topic_hr) + len(gar_assigned_to_topic_battery) + len(gar_assigned_to_topic_vo2) + len(gar_assigned_to_topic_price) \n",
    "             \n",
    "print(\"Number of sentences after validation:\", gar_sentences_after)\n",
    "\n",
    "# reporting number of sentences for each feature topic\n",
    "print(\"Number of sentences for gps:\", len(gar_assigned_to_topic_gps))\n",
    "print(\"Number of sentences for hr:\", len(gar_assigned_to_topic_hr))\n",
    "print(\"Number of sentences for battery:\", len(gar_assigned_to_topic_battery))\n",
    "print(\"Number of sentences for vo2:\", len(gar_assigned_to_topic_vo2))\n",
    "print(\"Number of sentences for price:\", len(gar_assigned_to_topic_price))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exporting data for Garmin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments after removing duplicates: 959\n",
      "Number of comments after removing duplicates: 849\n",
      "Number of sentences for gps: 225\n",
      "Number of sentences for hr: 291\n",
      "Number of sentences for battery: 223\n",
      "Number of sentences for vo2: 24\n",
      "Number of sentences for price: 86\n"
     ]
    }
   ],
   "source": [
    "# create dataframe for each list\n",
    "gar_gps_df = pd.DataFrame(gar_assigned_to_topic_gps, columns = ['comment'])\n",
    "gar_hr_df = pd.DataFrame(gar_assigned_to_topic_hr, columns = ['comment'])\n",
    "gar_battery_df = pd.DataFrame(gar_assigned_to_topic_battery, columns = ['comment'])\n",
    "gar_vo2_df = pd.DataFrame(gar_assigned_to_topic_vo2, columns = ['comment'])\n",
    "gar_price_df = pd.DataFrame(gar_assigned_to_topic_price, columns = ['comment'])\n",
    "\n",
    "# add label column for topic\n",
    "gar_gps_df['topic'] = 'gps'\n",
    "gar_hr_df['topic'] = 'hr'\n",
    "gar_battery_df['topic'] = 'battery'\n",
    "gar_vo2_df['topic'] = 'vo2'\n",
    "gar_price_df['topic'] = 'price'\n",
    "\n",
    "# join dataframes together\n",
    "frames_2 = [gar_gps_df,\n",
    "          gar_hr_df,\n",
    "          gar_battery_df,\n",
    "          gar_vo2_df,\n",
    "          gar_price_df]\n",
    "\n",
    "\n",
    "garmin_topic_comments = pd.concat(frames_2)\n",
    "\n",
    "# reporting number of sentences before removing duplicates\n",
    "print(\"Number of comments after removing duplicates:\", len(garmin_topic_comments))\n",
    "\n",
    "# removing duplicates to ensure one sentence is not assigned to more than one feature topic\n",
    "garmin_topic_comments.drop_duplicates(subset=['comment'], inplace = True)\n",
    "\n",
    "# reporting number of sentences after removing duplicates\n",
    "print(\"Number of comments after removing duplicates:\", len(garmin_topic_comments))\n",
    "\n",
    "# reporting number of sentences for each feature topic\n",
    "print(\"Number of sentences for gps:\", len(garmin_topic_comments[garmin_topic_comments['topic'] == 'gps']))\n",
    "print(\"Number of sentences for hr:\", len(garmin_topic_comments[garmin_topic_comments['topic'] == 'hr']))\n",
    "print(\"Number of sentences for battery:\", len(garmin_topic_comments[garmin_topic_comments['topic'] == 'battery']))\n",
    "print(\"Number of sentences for vo2:\", len(garmin_topic_comments[garmin_topic_comments['topic'] == 'vo2']))\n",
    "print(\"Number of sentences for price:\", len(garmin_topic_comments[garmin_topic_comments['topic'] == 'price']))\n",
    "\n",
    "# export to csv\n",
    "date = '2020-12-08'\n",
    "garmin_topic_comments.to_csv(date + '_garmin_topic_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3a. Sentiment scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature-related sentences for Apple: 772\n",
      "Number of feature-related sentences for Garmin: 849\n"
     ]
    }
   ],
   "source": [
    "# ------------------- GARMIN -------------------\n",
    "\n",
    "# load data\n",
    "garmin_df = pd.read_csv('2020-12-08_garmin_topic_comments.csv') \n",
    "\n",
    "# create empty dictionary for sentiment scores\n",
    "garmin_sentiment = {}\n",
    "\n",
    "# run loop to iterate through every review while applying sentiment scoring\n",
    "for i,x in enumerate(garmin_df.comment):\n",
    "    garmin_sentiment[i] = vader.polarity_scores(x)\n",
    "    \n",
    "# convert dictionary into dataframe\n",
    "garmin_sentiment = pd.DataFrame(garmin_sentiment)\n",
    "\n",
    "# transpose to get correct form\n",
    "garmin_sentiment = garmin_sentiment.transpose()\n",
    "\n",
    "# add index column to dataframes\n",
    "garmin_sentiment.reset_index(drop=True, inplace=True)\n",
    "garmin_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# join dataframesa together \n",
    "garmin_results = pd.concat([garmin_df, garmin_sentiment],  axis=1, ignore_index=False)\n",
    "\n",
    "# export to csv\n",
    "garmin_results.to_csv('03_Results/garmin_results.csv', index=False)\n",
    "\n",
    "\n",
    "# ------------------- APPLE -------------------\n",
    "\n",
    "# load data\n",
    "apple_df = pd.read_csv('2020-12-08_apple_topic_comments.csv') \n",
    "\n",
    "# create empty dictionary for sentiment scores\n",
    "apple_sentiment = {}\n",
    "\n",
    "# run loop to iterate through every review while applying sentiment scoring\n",
    "for i,x in enumerate(apple_df.comment):\n",
    "    apple_sentiment[i] = vader.polarity_scores(x)\n",
    "    \n",
    "# convert dictionary into dataframe\n",
    "apple_sentiment = pd.DataFrame(apple_sentiment)\n",
    "\n",
    "# transpose to get correct form\n",
    "apple_sentiment = apple_sentiment.transpose()\n",
    "\n",
    "# add index column to dataframes\n",
    "apple_sentiment.reset_index(drop=True, inplace=True)\n",
    "apple_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# join dataframesa together \n",
    "apple_results = pd.concat([apple_df, apple_sentiment],  axis=1, ignore_index=False)\n",
    "\n",
    "# export to csv\n",
    "apple_results.to_csv('03_Results/apple_results.csv', index=False)\n",
    "\n",
    "# reporting number of sentences for each brand to cross-check with previous data set\n",
    "print('Number of feature-related sentences for Apple:', len(apple_results))\n",
    "print('Number of feature-related sentences for Garmin:', len(garmin_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3] *",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
